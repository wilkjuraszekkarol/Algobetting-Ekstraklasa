{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c04b0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802bf585",
   "metadata": {},
   "source": [
    "First let's load our (already normalized) data, and specify input and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PATH\\\\wszystko_znormalizowane.csv\", encoding='utf-8-sig', index_col=[0])\n",
    "df = df.dropna() #Remove all rows with at least one element, just in case\n",
    "df = df.drop_duplicates() #Just in case\n",
    "custom_map = {'Wygrali gospodarze':0,'Remis':1,'Wygrali goście':2} #Rename target variable elements to integers so that these are compatible with classification models\n",
    "df['Kto_wygrał'] = df['Kto_wygrał'].map(custom_map)\n",
    "odds1 = df[['Oddsy_gospodarze', 'Oddsy_remis', 'Oddsy_goście', 'Kto_wygrał']] \n",
    "\n",
    "df_Xy = df.drop(df.columns[range(10)], axis=1)\n",
    "y = df_Xy['Kto_wygrał']\n",
    "X = df_Xy.drop(['Kto_wygrał'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a36df893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFG - Stałe Fragmenty Gry. Basically a percentage of how many goals each team scored from set pieces during the previous season. All data for each game is from the previous season, no more no less.\n",
    "# Other variable names are pretty much self-explanatory\n",
    "#df.head(30) #Uncomment if you want to take a look at what we're working with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a348172",
   "metadata": {},
   "source": [
    "Let's quickly check how much would we get back if we just gambled randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e056557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.39576522626719\n"
     ]
    }
   ],
   "source": [
    "tabela = odds1.copy()\n",
    "array = np.random.randint(3, size=2939)\n",
    "tabela.insert(4, \"Randomly_assigned\", array)\n",
    "tabela.head(10)\n",
    "rand_return_per_100 = 0\n",
    "for j in range(5000):\n",
    "    tabela = odds1.copy()\n",
    "    array = np.random.randint(3, size=2939)\n",
    "    tabela.insert(4, \"Randomly_assigned\", array)\n",
    "    tabela[\"Wygrana\"] = 0.0\n",
    "    tabela[\"Wygrana\"] = np.where(tabela[\"Kto_wygrał\"] == tabela[\"Randomly_assigned\"], 1, 0.0)\n",
    "    tabela.loc[tabela.Kto_wygrał == 0,'Wygrana'] = tabela.Oddsy_gospodarze * 100 * tabela.Wygrana\n",
    "    tabela.loc[tabela.Kto_wygrał == 1,'Wygrana'] = tabela.Oddsy_remis * 100 * tabela.Wygrana\n",
    "    tabela.loc[tabela.Kto_wygrał == 2,'Wygrana'] = tabela.Oddsy_goście * 100 * tabela.Wygrana\n",
    "    rand_return_per_100 = rand_return_per_100 + (tabela['Wygrana'].sum())/len(tabela['Wygrana'])\n",
    "print(rand_return_per_100/5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14902a73",
   "metadata": {},
   "source": [
    "Well that means that our model should on average bring at the very least ~94.30$ (per 100 invested)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f531f9",
   "metadata": {},
   "source": [
    "#### ExpectedReturnNaive algorithm picks scenario with the highest probability, and returns what we would get back by placing bets according to this strategy.\n",
    "\n",
    "Here we want to observe how betting on the most probable scenario will impact our returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bde937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExpectedReturnNaive(__Classifier__, __criterion__, odds, X, y, i, j, t, wyg, acc):\n",
    "    X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=t, random_state=j)\n",
    "    meczTree = __Classifier__(criterion=__criterion__, max_depth = i) #From my observations, criterion type's impact is extremely marginal\n",
    "    meczTree.fit(X_trainset,y_trainset) #Model training ^^^\n",
    "\n",
    "    clf_predictions = meczTree.predict(X_testset) \n",
    "    indices = list(X_testset.index.astype(int))\n",
    "    table = odds.loc[indices] #Makes sure that we actually allocate correct odds to a correct match\n",
    "    table['Predicted_Winner'] = pd.Series(clf_predictions)\n",
    "\n",
    "    table = table.dropna() #Somehow dropping NaN values twice wasn't enough\n",
    "    table['Predicted_Winner'] = table['Predicted_Winner'].astype(int)\n",
    "    table['Wygrana'] = 0.0\n",
    "    table.loc[table.Kto_wygrał == table.Predicted_Winner,'Wygrana'] = 1 #Prediction is correct => assign 1. It's incorrect => leave it at 0\n",
    "    table.loc[table.Kto_wygrał == 0,'Wygrana'] = table.Oddsy_gospodarze * 100 * table.Wygrana\n",
    "    table.loc[table.Kto_wygrał == 1,'Wygrana'] = table.Oddsy_remis * 100 * table.Wygrana\n",
    "    table.loc[table.Kto_wygrał == 2,'Wygrana'] = table.Oddsy_goście * 100 * table.Wygrana\n",
    "    \n",
    "    wyg = wyg + (table['Wygrana'].sum())/len(table['Wygrana']) #Average return per 100zł\n",
    "    acc = acc + 100*(round(metrics.accuracy_score(y_testset, clf_predictions),4)) #Percentage score of correct picks\n",
    "    return wyg, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216496a9",
   "metadata": {},
   "source": [
    "#### ExpectedReturnAdvanced algorithm calculates Expected Value of each bet (i.e. P(scenario A) x bookmaker odds for scenario A), picks the one with highest EV, and returns what it'd yield back per 100zł placed, over the course of 2015/16 - 2024/25 seasons.\n",
    "\n",
    "It's may not be the prettiest chunk of code, but tech debt isn't really an issue here. It's purpose is to give each class its own weight (i.e. odds of each scenario), for each sample respectively. Scikit-learn doesn't allow to allocate a dataframe of class weights (one for each observation), so we have to do it manually. Because of that, we can't really train a model to directly search for highest betting returns - however, if probability scores are accurate enough, that's... basically the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5572715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExpectedReturnAdvanced(__Classifier__, __criterion__, odds, X, y, i, j, t, wyg, acc):\n",
    "\n",
    "    X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=t, random_state=j)\n",
    "    meczTree = __Classifier__(criterion=__criterion__, max_depth = i)\n",
    "    meczTree.fit(X_trainset,y_trainset) #Model training ^^^\n",
    "\n",
    "    clf_predictions = meczTree.predict_proba(X_testset) #Notice how this time we are deriving exact probabilities of each scenario\n",
    "    clf_predictions = pd.DataFrame(clf_predictions)\n",
    "    indices = list(X_testset.index)\n",
    "    odds_tree_predictions = odds.loc[indices]\n",
    "    odds_tree_predictions = odds_tree_predictions.reset_index(drop=True)\n",
    "    table = odds_tree_predictions.merge(clf_predictions, left_index=True, right_index=True)\n",
    "\n",
    "    table['Potencjalny_return'] = 0\n",
    "    table['Potencjalny_return'] = np.where(table['Kto_wygrał'] == 0, table['Oddsy_gospodarze']*100, table['Potencjalny_return']) #Creates a separate column that stores how much we'd get back if we picked the winner correctly\n",
    "    table['Potencjalny_return'] = np.where(table['Kto_wygrał'] == 1, table['Oddsy_remis']*100, table['Potencjalny_return'])\n",
    "    table['Potencjalny_return'] = np.where(table['Kto_wygrał'] == 2, table['Oddsy_goście']*100, table['Potencjalny_return'])\n",
    "    table['Potencjalny_return'] = table['Potencjalny_return'].astype(int)\n",
    "    table['Return_gospodarze'] = table['Oddsy_gospodarze'] * table[0] #Return_A calculates EV of scenario A\n",
    "    table['Return_remis'] = table['Oddsy_remis'] * table[1] \n",
    "    table['Return_goście'] = table['Oddsy_goście'] * table[2]\n",
    "    table[\"Max(EV)\"] = table[['Return_gospodarze', 'Return_remis', 'Return_goście']].max(axis=1) #Takes the highest EV out of all scenarios (for each observation)\n",
    "    table['Max(EV)'] = np.where(table['Max(EV)'] == table['Return_gospodarze'], 0, table['Max(EV)']) #Picks which scenario in a given observation has the highest EV\n",
    "    table['Max(EV)'] = np.where(table['Max(EV)'] == table['Return_remis'], 1, table['Max(EV)']) \n",
    "    table['Max(EV)'] = np.where(table['Max(EV)'] == table['Return_goście'], 2, table['Max(EV)']) #In this model, we would bet exactly on the pick with highest EV\n",
    "    table['Max(EV)'] = table['Max(EV)'].astype(int)\n",
    "    table['Actual_return'] = np.where(table['Max(EV)'] == table['Kto_wygrał'], table['Potencjalny_return'], 0) #If scenario with max. EV = scenario that happened, allocate returns into here\n",
    "    \n",
    "    wyg = wyg + table['Actual_return'].sum() / len(table) #Average return per 100zł\n",
    "    acc = acc + 100*(np.count_nonzero(table['Actual_return']))/len(table['Actual_return']) #Percentage score of correct picks\n",
    "\n",
    "    return wyg, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff609cc",
   "metadata": {},
   "source": [
    "Now let's test how both approaches behave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba9e0d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable scenario | max_depth=1, test_size = 0.2 ==> Average return per 100zł: 97.767, Average accuracy score = 43.754%\n",
      "Highest EV scenario    | max_depth=1, test_size = 0.2 ==> Average return per 100zł: 92.839, Average accuracy score = 28.449%\n",
      "Most probable scenario | max_depth=2, test_size = 0.2 ==> Average return per 100zł: 96.774, Average accuracy score = 43.965%\n",
      "Highest EV scenario    | max_depth=2, test_size = 0.2 ==> Average return per 100zł: 92.365, Average accuracy score = 29.524%\n",
      "Most probable scenario | max_depth=3, test_size = 0.2 ==> Average return per 100zł: 98.052, Average accuracy score = 43.841%\n",
      "Highest EV scenario    | max_depth=3, test_size = 0.2 ==> Average return per 100zł: 91.818, Average accuracy score = 29.599%\n",
      "Most probable scenario | max_depth=4, test_size = 0.2 ==> Average return per 100zł: 97.829, Average accuracy score = 43.649%\n",
      "Highest EV scenario    | max_depth=4, test_size = 0.2 ==> Average return per 100zł: 91.689, Average accuracy score = 29.726%\n",
      "Most probable scenario | max_depth=5, test_size = 0.2 ==> Average return per 100zł: 98.102, Average accuracy score = 42.831%\n",
      "Highest EV scenario    | max_depth=5, test_size = 0.2 ==> Average return per 100zł: 90.724, Average accuracy score = 29.577%\n",
      "Most probable scenario | max_depth=6, test_size = 0.2 ==> Average return per 100zł: 100.204, Average accuracy score = 42.356%\n",
      "Highest EV scenario    | max_depth=6, test_size = 0.2 ==> Average return per 100zł: 90.946, Average accuracy score = 29.624%\n",
      "Most probable scenario | max_depth=7, test_size = 0.2 ==> Average return per 100zł: 100.019, Average accuracy score = 41.834%\n",
      "Highest EV scenario    | max_depth=7, test_size = 0.2 ==> Average return per 100zł: 90.443, Average accuracy score = 29.459%\n",
      "Most probable scenario | max_depth=8, test_size = 0.2 ==> Average return per 100zł: 98.996, Average accuracy score = 41.381%\n",
      "Highest EV scenario    | max_depth=8, test_size = 0.2 ==> Average return per 100zł: 90.873, Average accuracy score = 29.252%\n",
      "Most probable scenario | max_depth=9, test_size = 0.2 ==> Average return per 100zł: 97.491, Average accuracy score = 40.608%\n",
      "Highest EV scenario    | max_depth=9, test_size = 0.2 ==> Average return per 100zł: 90.222, Average accuracy score = 28.833%\n",
      "Most probable scenario | max_depth=10, test_size = 0.2 ==> Average return per 100zł: 98.479, Average accuracy score = 40.285%\n",
      "Highest EV scenario    | max_depth=10, test_size = 0.2 ==> Average return per 100zł: 90.708, Average accuracy score = 28.745%\n",
      "Most probable scenario | max_depth=11, test_size = 0.2 ==> Average return per 100zł: 99.179, Average accuracy score = 39.739%\n",
      "Highest EV scenario    | max_depth=11, test_size = 0.2 ==> Average return per 100zł: 90.800, Average accuracy score = 28.556%\n",
      "Most probable scenario | max_depth=12, test_size = 0.2 ==> Average return per 100zł: 98.729, Average accuracy score = 39.192%\n",
      "Highest EV scenario    | max_depth=12, test_size = 0.2 ==> Average return per 100zł: 90.351, Average accuracy score = 28.204%\n",
      "Most probable scenario | max_depth=13, test_size = 0.2 ==> Average return per 100zł: 96.346, Average accuracy score = 38.896%\n",
      "Highest EV scenario    | max_depth=13, test_size = 0.2 ==> Average return per 100zł: 89.611, Average accuracy score = 27.776%\n",
      "Most probable scenario | max_depth=14, test_size = 0.2 ==> Average return per 100zł: 96.066, Average accuracy score = 38.602%\n",
      "Highest EV scenario    | max_depth=14, test_size = 0.2 ==> Average return per 100zł: 90.134, Average accuracy score = 27.811%\n",
      "Most probable scenario | max_depth=15, test_size = 0.2 ==> Average return per 100zł: 97.615, Average accuracy score = 38.329%\n",
      "Highest EV scenario    | max_depth=15, test_size = 0.2 ==> Average return per 100zł: 90.303, Average accuracy score = 27.764%\n",
      "Most probable scenario | max_depth=16, test_size = 0.2 ==> Average return per 100zł: 97.238, Average accuracy score = 38.246%\n",
      "Highest EV scenario    | max_depth=16, test_size = 0.2 ==> Average return per 100zł: 90.367, Average accuracy score = 27.707%\n",
      "Most probable scenario | max_depth=17, test_size = 0.2 ==> Average return per 100zł: 96.994, Average accuracy score = 37.866%\n",
      "Highest EV scenario    | max_depth=17, test_size = 0.2 ==> Average return per 100zł: 89.744, Average accuracy score = 27.461%\n",
      "Most probable scenario | max_depth=18, test_size = 0.2 ==> Average return per 100zł: 96.804, Average accuracy score = 37.756%\n",
      "Highest EV scenario    | max_depth=18, test_size = 0.2 ==> Average return per 100zł: 89.864, Average accuracy score = 27.510%\n",
      "Most probable scenario | max_depth=19, test_size = 0.2 ==> Average return per 100zł: 96.213, Average accuracy score = 37.621%\n",
      "Highest EV scenario    | max_depth=19, test_size = 0.2 ==> Average return per 100zł: 89.523, Average accuracy score = 27.347%\n",
      "Most probable scenario | max_depth=20, test_size = 0.2 ==> Average return per 100zł: 96.471, Average accuracy score = 37.543%\n",
      "Highest EV scenario    | max_depth=20, test_size = 0.2 ==> Average return per 100zł: 89.586, Average accuracy score = 27.327%\n",
      "Most probable scenario | max_depth=21, test_size = 0.2 ==> Average return per 100zł: 96.132, Average accuracy score = 37.460%\n",
      "Highest EV scenario    | max_depth=21, test_size = 0.2 ==> Average return per 100zł: 89.099, Average accuracy score = 27.177%\n",
      "Most probable scenario | max_depth=22, test_size = 0.2 ==> Average return per 100zł: 96.878, Average accuracy score = 37.537%\n",
      "Highest EV scenario    | max_depth=22, test_size = 0.2 ==> Average return per 100zł: 89.214, Average accuracy score = 27.197%\n",
      "Most probable scenario | max_depth=23, test_size = 0.2 ==> Average return per 100zł: 96.337, Average accuracy score = 37.522%\n",
      "Highest EV scenario    | max_depth=23, test_size = 0.2 ==> Average return per 100zł: 89.329, Average accuracy score = 27.250%\n",
      "Most probable scenario | max_depth=24, test_size = 0.2 ==> Average return per 100zł: 96.380, Average accuracy score = 37.309%\n",
      "Highest EV scenario    | max_depth=24, test_size = 0.2 ==> Average return per 100zł: 89.971, Average accuracy score = 27.415%\n",
      "Most probable scenario | max_depth=25, test_size = 0.2 ==> Average return per 100zł: 95.872, Average accuracy score = 37.289%\n",
      "Highest EV scenario    | max_depth=25, test_size = 0.2 ==> Average return per 100zł: 89.756, Average accuracy score = 27.361%\n",
      "Most probable scenario | max_depth=26, test_size = 0.2 ==> Average return per 100zł: 97.023, Average accuracy score = 37.265%\n",
      "Highest EV scenario    | max_depth=26, test_size = 0.2 ==> Average return per 100zł: 89.327, Average accuracy score = 27.192%\n",
      "Most probable scenario | max_depth=27, test_size = 0.2 ==> Average return per 100zł: 96.785, Average accuracy score = 37.218%\n",
      "Highest EV scenario    | max_depth=27, test_size = 0.2 ==> Average return per 100zł: 89.605, Average accuracy score = 27.272%\n",
      "Most probable scenario | max_depth=28, test_size = 0.2 ==> Average return per 100zł: 96.541, Average accuracy score = 37.343%\n",
      "Highest EV scenario    | max_depth=28, test_size = 0.2 ==> Average return per 100zł: 89.301, Average accuracy score = 27.204%\n",
      "Most probable scenario | max_depth=29, test_size = 0.2 ==> Average return per 100zł: 96.679, Average accuracy score = 37.338%\n",
      "Highest EV scenario    | max_depth=29, test_size = 0.2 ==> Average return per 100zł: 89.164, Average accuracy score = 27.158%\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "n = 100\n",
    "for i in range(1,30):\n",
    "    return1, return2 = (0,0)\n",
    "    accuracy1, accuracy2 = (0,0)\n",
    "    for j in range(n):\n",
    "        return1, accuracy1 = ExpectedReturnNaive(DecisionTreeClassifier, \"entropy\", odds1, X, y, i, j, test_size, return1, accuracy1)\n",
    "    print(f\"Most probable scenario | max_depth={i}, test_size = {test_size} ==> Average return per 100zł: {(return1/n):.3f}, Average accuracy score = {(accuracy1/n):.3f}%\") \n",
    "    for j in range(n):\n",
    "        return2, accuracy2 = ExpectedReturnAdvanced(DecisionTreeClassifier, \"entropy\", odds1, X, y, i, j, test_size, return2, accuracy2)\n",
    "    print(f\"Highest EV scenario    | max_depth={i}, test_size = {test_size} ==> Average return per 100zł: {(return2/n):.3f}, Average accuracy score = {(accuracy2/n):.3f}%\")\n",
    "#Here we're defining a random_state, because we want reproducibility - crucial for cross examination.\n",
    "#We repeat the algorithm n times, because, well, we don't want to hop onto a bad model just because it got a lucky test sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959ce6c",
   "metadata": {},
   "source": [
    "As we can see, the \"naive\" solution doesn't do that bad - it's not profitable, but it performs better than average, especially for max_depth between 3 and 12. However, the supposedly 'advanced\" algorithm performs horrifyingly badly.\n",
    "\n",
    "How come?\n",
    "- We are betting on less probable scenarios, but ones that wield larger returns. Hence, its' accuracy score is (and should be) below 33%; the issue may be that it shouldn't be that low (it oscilates between 27 and 30%)\n",
    "- Or maybe the probability scores are inaccurate. If a scenario with odds = 12.00 has a 20% chance of occuring, we should definitely bet on it. But 5%? That's why these values matter.\n",
    "\n",
    "###### (oh and btw, if you're curious - high values for max_depth between 6 and 8 is due to variance; I checked it for n=1000 and its' return sits around 97%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c63e9",
   "metadata": {},
   "source": [
    "#### Let's try other models: This time, we'll go with a Random Forest. Since it takes way longer to compute, we won't make as many repetitions this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82bca3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable scenario | max_depth=1, test_size = 0.25 ==> Average return per 100zł: 98.683, Average accuracy score = 43.982%\n",
      "Most probable scenario | max_depth=2, test_size = 0.25 ==> Average return per 100zł: 98.377, Average accuracy score = 44.388%\n",
      "Most probable scenario | max_depth=3, test_size = 0.25 ==> Average return per 100zł: 97.430, Average accuracy score = 44.895%\n",
      "Most probable scenario | max_depth=4, test_size = 0.25 ==> Average return per 100zł: 97.149, Average accuracy score = 44.993%\n",
      "Most probable scenario | max_depth=5, test_size = 0.25 ==> Average return per 100zł: 96.825, Average accuracy score = 44.881%\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.25\n",
    "n = 100\n",
    "for i in range(1,6):\n",
    "    return1, return2 = (0,0)\n",
    "    accuracy1, accuracy2 = (0,0)\n",
    "    for j in range(n):\n",
    "        return1, accuracy1 = ExpectedReturnNaive(RandomForestClassifier, \"entropy\", odds1, X, y, i, j, test_size, return1, accuracy1)\n",
    "        #return2, accuracy2 = ExpectedReturnAdvanced(RandomForestClassifier, \"entropy\", odds1, X, y, i, j, test_size, return2, accuracy2)\n",
    "    print(f\"Most probable scenario | max_depth={i}, test_size = {test_size} ==> Average return per 100zł: {(return1/n):.3f}, Average accuracy score = {(accuracy1/n):.3f}%\") \n",
    "    #print(f\"Highest EV scenario    | max_depth={i}, test_size = {test_size} ==> Average return per 100zł: {(return2/n):.3f}, Average accuracy score = {(accuracy2/n):.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7a01f",
   "metadata": {},
   "source": [
    "Well that... doesn't look too bad. Still not profitable, but the test size and n repetitions are so large here that variance should be miniscule, and yet it's still not that far off from profitability. I mean, if you pick \"correct\" parameters, like n=20 and test_size = 0.05, you may find a 16% edge - potentially effective for convincing shareholders that your ideas are very good, a bit less efficient for holding onto your job.\n",
    "\n",
    "If you want to learn why I commented out the \"advanced\" algorithm, you can uncomment it and find out yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc23e8b6",
   "metadata": {},
   "source": [
    "#### Let's now check how XGBoost performs. It has a different syntax, so we can't just copy previously defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9557ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoostPredictionsNaive(oddsy, X, y, t, x, i, j, wyg, acc):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=t, random_state=j)\n",
    "    xgb_train = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "    xgb_test = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n",
    "    params = {\n",
    "        'objective': 'multi:softmax', #Puts observations into one of 3 categories\n",
    "        'max_depth': i,\n",
    "        'learning_rate': x,\n",
    "        'num_class': 3\n",
    "    }\n",
    "    model = xgb.train(params=params, dtrain=xgb_train, num_boost_round=50)\n",
    "    xgb_predictions = model.predict(xgb_test)\n",
    "    xgb_predictions = xgb_predictions.astype(int)\n",
    "    indices = list(X_test.index)\n",
    "    oddsy_xgb_predictions = oddsy.loc[indices]\n",
    "    oddsy_xgb_predictions = oddsy_xgb_predictions.reset_index(drop=True)\n",
    "    oddsy_xgb_predictions.insert(4, 'Predicted_Winner', xgb_predictions)\n",
    "    tabelka = oddsy_xgb_predictions\n",
    "    tabelka.loc[tabelka.Kto_wygrał == tabelka.Predicted_Winner,'Wygrana'] = 1\n",
    "    tabelka.loc[tabelka.Kto_wygrał == 0,'Wygrana'] = tabelka.Oddsy_gospodarze * 100 * tabelka.Wygrana\n",
    "    tabelka.loc[tabelka.Kto_wygrał == 1,'Wygrana'] = tabelka.Oddsy_remis * 100 * tabelka.Wygrana\n",
    "    tabelka.loc[tabelka.Kto_wygrał == 2,'Wygrana'] = tabelka.Oddsy_goście * 100 * tabelka.Wygrana\n",
    "    tabelka['Wygrana'] = tabelka['Wygrana'].fillna(0)\n",
    "    tabelka['Wygrana'] = tabelka['Wygrana'].astype(int)\n",
    "\n",
    "    wyg = wyg + (tabelka['Wygrana'].sum())/(len(tabelka['Wygrana']))\n",
    "    acc = acc + 100*(np.count_nonzero(tabelka['Wygrana']))/len(tabelka['Wygrana'])\n",
    "\n",
    "    return wyg, acc\n",
    "\n",
    "\n",
    "\n",
    "def XGBoostPredictionsAdvanced(oddsy, X, y, t, x, i, j, wyg, acc): #Let's try again\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=t, random_state=j)\n",
    "    xgb_train = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "    xgb_test = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n",
    "    params = {\n",
    "        'objective': 'multi:softprob', #Predicts a probability score of each scenario\n",
    "        'max_depth': i,\n",
    "        'learning_rate': x,\n",
    "        'num_class': 3\n",
    "    }\n",
    "    model = xgb.train(params=params, dtrain=xgb_train, num_boost_round=50)\n",
    "    xgb_predictions = model.predict(xgb_test)\n",
    "    xgb_predictions = pd.DataFrame(xgb_predictions)\n",
    "    indices = list(X_test.index)\n",
    "    oddsy_xgb_predictions = oddsy.loc[indices]\n",
    "    oddsy_xgb_predictions = oddsy_xgb_predictions.reset_index(drop=True)\n",
    "    tabelka = oddsy_xgb_predictions.merge(xgb_predictions, left_index=True, right_index=True)\n",
    "    tabelka['Potencjalny_return'] = 0 #I thought of putting everything below this part into a separate function, but it caused more issues than I was bothered to fix. Maybe later. It's not necessary, not if we don't have to maintain too much of this code\n",
    "    tabelka['Potencjalny_return'] = np.where(tabelka['Kto_wygrał'] == 0, tabelka['Oddsy_gospodarze']*100, tabelka['Potencjalny_return'])\n",
    "    tabelka['Potencjalny_return'] = np.where(tabelka['Kto_wygrał'] == 1, tabelka['Oddsy_remis']*100, tabelka['Potencjalny_return'])\n",
    "    tabelka['Potencjalny_return'] = np.where(tabelka['Kto_wygrał'] == 2, tabelka['Oddsy_goście']*100, tabelka['Potencjalny_return'])\n",
    "    tabelka['Potencjalny_return'] = tabelka['Potencjalny_return'].astype(int)\n",
    "    tabelka['Return_gospodarze'] = tabelka['Oddsy_gospodarze'] * tabelka[0]\n",
    "    tabelka['Return_remis'] = tabelka['Oddsy_remis'] * tabelka[1]\n",
    "    tabelka['Return_goście'] = tabelka['Oddsy_goście'] * tabelka[2]\n",
    "    tabelka[\"Max(EV)\"] = tabelka[['Return_gospodarze', 'Return_remis', 'Return_goście']].max(axis=1)\n",
    "    tabelka['Max(EV)'] = np.where(tabelka['Max(EV)'] == tabelka['Return_gospodarze'], 0, tabelka['Max(EV)'])\n",
    "    tabelka['Max(EV)'] = np.where(tabelka['Max(EV)'] == tabelka['Return_remis'], 1, tabelka['Max(EV)'])\n",
    "    tabelka['Max(EV)'] = np.where(tabelka['Max(EV)'] == tabelka['Return_goście'], 2, tabelka['Max(EV)'])\n",
    "    tabelka['Max(EV)'] = tabelka['Max(EV)'].astype(int)\n",
    "    tabelka['Actual_return'] = np.where(tabelka['Max(EV)'] == tabelka['Kto_wygrał'], tabelka['Potencjalny_return'], 0)\n",
    "    return_calkowity = tabelka['Actual_return'].sum()\n",
    "    wyg = wyg + return_calkowity / len(tabelka)\n",
    "    acc = acc + 100*(np.count_nonzero(tabelka['Actual_return']))/len(tabelka['Actual_return'])\n",
    "\n",
    "    return wyg, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c02a5a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoostPredictionsNaive    | Average return per 100zł for test_size = 0.2, learning_rate = 0.001, max_depth = 5:  99.068zł,  with accuracy_score = 44.310%\n",
      "XGBoostPredictionsAdvanced | Average return per 100zł for test_size = 0.2, learning_rate = 0.001, max_depth = 5:  90.995zł,  with accuracy_score = 24.680%\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "n = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "#for i in range(1,6):    Not needed when learning_rate is so low; other values will just yield the same result. We can tune other parameters as well, but that will most likely keep us around 99%\n",
    "wygrana1, wygrana2 = (0,0)\n",
    "accuracy1, accuracy2 = (0,0)\n",
    "for j in range(n):\n",
    "    wygrana1, accuracy1 = XGBoostPredictionsNaive(odds1, X, y, test_size, learning_rate, 1, j, wygrana1, accuracy1)\n",
    "    wygrana2, accuracy2 = XGBoostPredictionsAdvanced(odds1, X, y, test_size, learning_rate, 1, j, wygrana2, accuracy2)\n",
    "print(f\"XGBoostPredictionsNaive    | Average return per 100zł for test_size = {test_size}, learning_rate = {learning_rate}, max_depth = {i}:  {wygrana1/n:.3f}zł,  with accuracy_score = {accuracy1/n:.3f}%\")\n",
    "print(f\"XGBoostPredictionsAdvanced | Average return per 100zł for test_size = {test_size}, learning_rate = {learning_rate}, max_depth = {i}:  {wygrana2/n:.3f}zł,  with accuracy_score = {accuracy2/n:.3f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f9d04d",
   "metadata": {},
   "source": [
    "Well that's just embarassing for the so-called \"advanced\" algorithm. Percentage scores must be totally off.\n",
    "\n",
    "At the same time, again, the \"naive\" algorithm edges close to profitability, but it's so close yet so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8f2844",
   "metadata": {},
   "source": [
    "There's also LogisticRegression that was worth trying out, but honestly, results are comparable to how decision trees performed, i.e. that would bring exactly nothing to the table.\n",
    "\"Naive\" score oscilates around 97%, whereas \"advanced\" sits close to 93%. Again, not even better than random selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c002184",
   "metadata": {},
   "source": [
    "#### Some conclusions:\n",
    "- The \"naive\" algorithm is... not bad. 99% return doesn't sound too impressive, but keep in mind that EV of playing this game is 94.3%, meaning that if this were a zero-sum game, this algorithm would actually find a 5% edge.\n",
    " Especially impressive after considering the fact that the only data we are working with here, is each team's performance during the previous season. That's it.\n",
    "- If this model turned out to be profitable, then it'd be exceptionally easy to maintain. Data insertion would only have to occur once per year.\n",
    "- The \"advanced\" algorithm is obviously bad. Something must be off with percentage scores, it does matter if a 40%, 35%, 25% triplet of scenarios is predicted to be a 45%, 40%, 15%. We can see it by how low the accuracy score is; to some extent, it should be lower than 33%, because maximizing EV means that sometimes we should bet for the underdogs. But it can't be as low as 24%.\n",
    "- I've attempted to calibrate percentage scores using a CalibratedClassifierCV, but with little result. I might come back later and try again.\n",
    "- Don't gamble. Unless you have a verifiably profitable model. Then do gamble. It might turn out to be a safer investment than buying MAG-7 stocks (maybe not)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
