{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04b0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802bf585",
   "metadata": {},
   "source": [
    "First let's load our (already normalized) data, and specify input and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f3bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PATH\\\\wszystko_znormalizowane.csv\", encoding='utf-8-sig', index_col=[0]) #----CHANGE PATH----\n",
    "df = df.dropna() #Remove all rows with at least one element, just in case\n",
    "df = df.drop_duplicates() #Just in case\n",
    "custom_map = {'Wygrali gospodarze':0,'Remis':1,'Wygrali goście':2} #Rename target variable elements to integers so that these are compatible with classification models\n",
    "df['Kto_wygrał'] = df['Kto_wygrał'].map(custom_map)\n",
    "odds1 = df[['Oddsy_gospodarze', 'Oddsy_remis', 'Oddsy_goście', 'Kto_wygrał']] \n",
    "\n",
    "df_Xy = df.drop(df.columns[range(10)], axis=1)\n",
    "y = df_Xy['Kto_wygrał']\n",
    "X = df_Xy.drop(['Kto_wygrał'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36df893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFG - Stałe Fragmenty Gry. Basically a percentage of how many goals each team scored from set pieces during the previous season. All data for each game is from the previous season, no more no less.\n",
    "# Other variable names are pretty much self-explanatory\n",
    "#df.head(30) #Uncomment if you want to take a look at what we're working with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a348172",
   "metadata": {},
   "source": [
    "Let's quickly check how much would we get back if we just gambled randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e056557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.31924518543735\n"
     ]
    }
   ],
   "source": [
    "tabela = odds1.copy()\n",
    "array = np.random.randint(3, size=2939)\n",
    "tabela.insert(4, \"Randomly_assigned\", array)\n",
    "tabela.head(10)\n",
    "rand_return_per_100 = 0\n",
    "for j in range(5000):\n",
    "    tabela = odds1.copy()\n",
    "    array = np.random.randint(3, size=2939)\n",
    "    tabela.insert(4, \"Randomly_assigned\", array)\n",
    "    tabela[\"Wygrana\"] = 0.0\n",
    "    tabela[\"Wygrana\"] = np.where(tabela[\"Kto_wygrał\"] == tabela[\"Randomly_assigned\"], 1, 0.0)\n",
    "    tabela.loc[tabela.Kto_wygrał == 0,'Wygrana'] = tabela.Oddsy_gospodarze * 100 * tabela.Wygrana\n",
    "    tabela.loc[tabela.Kto_wygrał == 1,'Wygrana'] = tabela.Oddsy_remis * 100 * tabela.Wygrana\n",
    "    tabela.loc[tabela.Kto_wygrał == 2,'Wygrana'] = tabela.Oddsy_goście * 100 * tabela.Wygrana\n",
    "    rand_return_per_100 = rand_return_per_100 + (tabela['Wygrana'].sum())/len(tabela['Wygrana'])\n",
    "print(rand_return_per_100/5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14902a73",
   "metadata": {},
   "source": [
    "Well that means that our model should on average bring at the very least ~94.30$ (per 100 invested)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f531f9",
   "metadata": {},
   "source": [
    "#### ExpectedReturnNaive algorithm picks scenario with the highest probability, and returns what we would get back by placing bets according to this strategy.\n",
    "\n",
    "Here we want to observe how betting on the most probable scenario will impact our returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8bde937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExpectedReturnNaive(__Classifier__, __criterion__, odds, X, y, i, j, t, wyg, acc):\n",
    "    X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=t, random_state=j)\n",
    "    meczTree = __Classifier__(criterion=__criterion__, max_depth = i) #From my observations, criterion type's impact is extremely marginal\n",
    "    meczTree.fit(X_trainset,y_trainset) #Model training ^^^\n",
    "\n",
    "    clf_predictions = meczTree.predict(X_testset) \n",
    "    indices = list(X_testset.index.astype(int))\n",
    "    table = odds.loc[indices] #Makes sure that we actually allocate correct odds to a correct match\n",
    "    table['Predicted_Winner'] = pd.Series(clf_predictions)\n",
    "\n",
    "    table = table.dropna() #Somehow dropping NaN values twice wasn't enough\n",
    "    table['Predicted_Winner'] = table['Predicted_Winner'].astype(int)\n",
    "    table['Wygrana'] = 0.0\n",
    "    table.loc[table.Kto_wygrał == table.Predicted_Winner,'Wygrana'] = 1 #Prediction is correct => assign 1. It's incorrect => leave it at 0\n",
    "    table.loc[table.Kto_wygrał == 0,'Wygrana'] = table.Oddsy_gospodarze * 100 * table.Wygrana\n",
    "    table.loc[table.Kto_wygrał == 1,'Wygrana'] = table.Oddsy_remis * 100 * table.Wygrana\n",
    "    table.loc[table.Kto_wygrał == 2,'Wygrana'] = table.Oddsy_goście * 100 * table.Wygrana\n",
    "    \n",
    "    wyg = wyg + (table['Wygrana'].sum())/len(table['Wygrana']) #Average return per 100zł\n",
    "    acc = acc + 100*(round(metrics.accuracy_score(y_testset, clf_predictions),4)) #Percentage score of correct picks\n",
    "    return wyg, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216496a9",
   "metadata": {},
   "source": [
    "#### ExpectedReturnAdvanced algorithm calculates Expected Value of each bet (i.e. P(scenario A) x bookmaker odds for scenario A), picks the one with highest EV, and returns what it'd yield back per 100zł placed, over the course of 2015/16 - 2024/25 seasons.\n",
    "\n",
    "It's may not be the prettiest chunk of code, but tech debt isn't really an issue here. Its purpose is to give each class its own weight (i.e. odds of each scenario), for each sample respectively. Scikit-learn doesn't allow to allocate a dataframe of class weights (one for each observation), so we have to do it manually. Because of that, we can't really train a model to directly search for highest betting returns - however, if probability scores are accurate enough, that's... basically the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5572715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExpectedReturnAdvanced(__Classifier__, __criterion__, odds, X, y, i, j, t, wyg, acc):\n",
    "\n",
    "    X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=t, random_state=j)\n",
    "    meczTree = __Classifier__(criterion=__criterion__, max_depth = i)\n",
    "    meczTree.fit(X_trainset,y_trainset) #Model training ^^^\n",
    "\n",
    "    clf_predictions = meczTree.predict_proba(X_testset) #Notice how this time we are deriving exact probabilities of each scenario\n",
    "    clf_predictions = pd.DataFrame(clf_predictions)\n",
    "    indices = list(X_testset.index)\n",
    "    odds_tree_predictions = odds.loc[indices]\n",
    "    odds_tree_predictions = odds_tree_predictions.reset_index(drop=True)\n",
    "    table = odds_tree_predictions.merge(clf_predictions, left_index=True, right_index=True)\n",
    "\n",
    "    table['Potencjalny_return'] = 0\n",
    "    table['Potencjalny_return'] = np.where(table['Kto_wygrał'] == 0, table['Oddsy_gospodarze']*100, table['Potencjalny_return']) #Creates a separate column that stores how much we'd get back if we picked the winner correctly\n",
    "    table['Potencjalny_return'] = np.where(table['Kto_wygrał'] == 1, table['Oddsy_remis']*100, table['Potencjalny_return'])\n",
    "    table['Potencjalny_return'] = np.where(table['Kto_wygrał'] == 2, table['Oddsy_goście']*100, table['Potencjalny_return'])\n",
    "    table['Potencjalny_return'] = table['Potencjalny_return'].astype(int)\n",
    "    table['Return_gospodarze'] = table['Oddsy_gospodarze'] * table[0] #Return_A calculates EV of scenario A\n",
    "    table['Return_remis'] = table['Oddsy_remis'] * table[1] \n",
    "    table['Return_goście'] = table['Oddsy_goście'] * table[2]\n",
    "    table[\"Max(EV)\"] = table[['Return_gospodarze', 'Return_remis', 'Return_goście']].max(axis=1) #Takes the highest EV out of all scenarios (for each observation)\n",
    "    table['Max(EV)'] = np.where(table['Max(EV)'] == table['Return_gospodarze'], 0, table['Max(EV)']) #Picks which scenario in a given observation has the highest EV\n",
    "    table['Max(EV)'] = np.where(table['Max(EV)'] == table['Return_remis'], 1, table['Max(EV)']) \n",
    "    table['Max(EV)'] = np.where(table['Max(EV)'] == table['Return_goście'], 2, table['Max(EV)']) #In this model, we would bet exactly on the pick with highest EV\n",
    "    table['Max(EV)'] = table['Max(EV)'].astype(int)\n",
    "    table['Actual_return'] = np.where(table['Max(EV)'] == table['Kto_wygrał'], table['Potencjalny_return'], 0) #If scenario with max. EV = scenario that happened, allocate returns into here\n",
    "    \n",
    "    wyg = wyg + table['Actual_return'].sum() / len(table) #Average return per 100zł\n",
    "    acc = acc + 100*(np.count_nonzero(table['Actual_return']))/len(table['Actual_return']) #Percentage score of correct picks\n",
    "\n",
    "    return wyg, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff609cc",
   "metadata": {},
   "source": [
    "Now let's test how both approaches behave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ba9e0d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable scenario | max_depth=1, test_size = 0.2 ==> Average return per 100zł: 97.767, Average accuracy score = 43.754%\n",
      "Highest EV scenario    | max_depth=1, test_size = 0.2 ==> Average return per 100zł: 92.839, Average accuracy score = 28.449%\n",
      "Most probable scenario | max_depth=2, test_size = 0.2 ==> Average return per 100zł: 96.774, Average accuracy score = 43.965%\n",
      "Highest EV scenario    | max_depth=2, test_size = 0.2 ==> Average return per 100zł: 92.365, Average accuracy score = 29.524%\n",
      "Most probable scenario | max_depth=3, test_size = 0.2 ==> Average return per 100zł: 98.052, Average accuracy score = 43.843%\n",
      "Highest EV scenario    | max_depth=3, test_size = 0.2 ==> Average return per 100zł: 91.810, Average accuracy score = 29.597%\n",
      "Most probable scenario | max_depth=4, test_size = 0.2 ==> Average return per 100zł: 97.837, Average accuracy score = 43.654%\n",
      "Highest EV scenario    | max_depth=4, test_size = 0.2 ==> Average return per 100zł: 91.704, Average accuracy score = 29.726%\n",
      "Most probable scenario | max_depth=5, test_size = 0.2 ==> Average return per 100zł: 98.044, Average accuracy score = 42.826%\n",
      "Highest EV scenario    | max_depth=5, test_size = 0.2 ==> Average return per 100zł: 90.696, Average accuracy score = 29.571%\n",
      "Most probable scenario | max_depth=6, test_size = 0.2 ==> Average return per 100zł: 100.048, Average accuracy score = 42.338%\n",
      "Highest EV scenario    | max_depth=6, test_size = 0.2 ==> Average return per 100zł: 90.928, Average accuracy score = 29.617%\n",
      "Most probable scenario | max_depth=7, test_size = 0.2 ==> Average return per 100zł: 100.271, Average accuracy score = 41.862%\n",
      "Highest EV scenario    | max_depth=7, test_size = 0.2 ==> Average return per 100zł: 90.514, Average accuracy score = 29.474%\n",
      "Most probable scenario | max_depth=8, test_size = 0.2 ==> Average return per 100zł: 98.947, Average accuracy score = 41.369%\n",
      "Highest EV scenario    | max_depth=8, test_size = 0.2 ==> Average return per 100zł: 90.935, Average accuracy score = 29.270%\n",
      "Most probable scenario | max_depth=9, test_size = 0.2 ==> Average return per 100zł: 97.862, Average accuracy score = 40.658%\n",
      "Highest EV scenario    | max_depth=9, test_size = 0.2 ==> Average return per 100zł: 90.340, Average accuracy score = 28.888%\n",
      "Most probable scenario | max_depth=10, test_size = 0.2 ==> Average return per 100zł: 98.828, Average accuracy score = 40.464%\n",
      "Highest EV scenario    | max_depth=10, test_size = 0.2 ==> Average return per 100zł: 90.537, Average accuracy score = 28.718%\n",
      "Most probable scenario | max_depth=11, test_size = 0.2 ==> Average return per 100zł: 98.808, Average accuracy score = 39.781%\n",
      "Highest EV scenario    | max_depth=11, test_size = 0.2 ==> Average return per 100zł: 90.949, Average accuracy score = 28.585%\n",
      "Most probable scenario | max_depth=12, test_size = 0.2 ==> Average return per 100zł: 99.251, Average accuracy score = 39.087%\n",
      "Highest EV scenario    | max_depth=12, test_size = 0.2 ==> Average return per 100zł: 90.495, Average accuracy score = 28.252%\n",
      "Most probable scenario | max_depth=13, test_size = 0.2 ==> Average return per 100zł: 96.829, Average accuracy score = 38.883%\n",
      "Highest EV scenario    | max_depth=13, test_size = 0.2 ==> Average return per 100zł: 89.747, Average accuracy score = 27.820%\n",
      "Most probable scenario | max_depth=14, test_size = 0.2 ==> Average return per 100zł: 96.858, Average accuracy score = 38.559%\n",
      "Highest EV scenario    | max_depth=14, test_size = 0.2 ==> Average return per 100zł: 90.216, Average accuracy score = 27.883%\n",
      "Most probable scenario | max_depth=15, test_size = 0.2 ==> Average return per 100zł: 97.311, Average accuracy score = 38.352%\n",
      "Highest EV scenario    | max_depth=15, test_size = 0.2 ==> Average return per 100zł: 90.263, Average accuracy score = 27.769%\n",
      "Most probable scenario | max_depth=16, test_size = 0.2 ==> Average return per 100zł: 96.685, Average accuracy score = 38.084%\n",
      "Highest EV scenario    | max_depth=16, test_size = 0.2 ==> Average return per 100zł: 90.294, Average accuracy score = 27.714%\n",
      "Most probable scenario | max_depth=17, test_size = 0.2 ==> Average return per 100zł: 97.119, Average accuracy score = 37.852%\n",
      "Highest EV scenario    | max_depth=17, test_size = 0.2 ==> Average return per 100zł: 89.889, Average accuracy score = 27.524%\n",
      "Most probable scenario | max_depth=18, test_size = 0.2 ==> Average return per 100zł: 97.177, Average accuracy score = 37.715%\n",
      "Highest EV scenario    | max_depth=18, test_size = 0.2 ==> Average return per 100zł: 89.914, Average accuracy score = 27.478%\n",
      "Most probable scenario | max_depth=19, test_size = 0.2 ==> Average return per 100zł: 96.744, Average accuracy score = 37.532%\n",
      "Highest EV scenario    | max_depth=19, test_size = 0.2 ==> Average return per 100zł: 89.362, Average accuracy score = 27.286%\n",
      "Most probable scenario | max_depth=20, test_size = 0.2 ==> Average return per 100zł: 96.662, Average accuracy score = 37.483%\n",
      "Highest EV scenario    | max_depth=20, test_size = 0.2 ==> Average return per 100zł: 89.450, Average accuracy score = 27.294%\n",
      "Most probable scenario | max_depth=21, test_size = 0.2 ==> Average return per 100zł: 95.813, Average accuracy score = 37.403%\n",
      "Highest EV scenario    | max_depth=21, test_size = 0.2 ==> Average return per 100zł: 89.510, Average accuracy score = 27.262%\n",
      "Most probable scenario | max_depth=22, test_size = 0.2 ==> Average return per 100zł: 96.920, Average accuracy score = 37.488%\n",
      "Highest EV scenario    | max_depth=22, test_size = 0.2 ==> Average return per 100zł: 89.263, Average accuracy score = 27.219%\n",
      "Most probable scenario | max_depth=23, test_size = 0.2 ==> Average return per 100zł: 96.160, Average accuracy score = 37.549%\n",
      "Highest EV scenario    | max_depth=23, test_size = 0.2 ==> Average return per 100zł: 89.520, Average accuracy score = 27.262%\n",
      "Most probable scenario | max_depth=24, test_size = 0.2 ==> Average return per 100zł: 96.408, Average accuracy score = 37.412%\n",
      "Highest EV scenario    | max_depth=24, test_size = 0.2 ==> Average return per 100zł: 89.945, Average accuracy score = 27.379%\n",
      "Most probable scenario | max_depth=25, test_size = 0.2 ==> Average return per 100zł: 96.025, Average accuracy score = 37.360%\n",
      "Highest EV scenario    | max_depth=25, test_size = 0.2 ==> Average return per 100zł: 89.499, Average accuracy score = 27.233%\n",
      "Most probable scenario | max_depth=26, test_size = 0.2 ==> Average return per 100zł: 97.140, Average accuracy score = 37.240%\n",
      "Highest EV scenario    | max_depth=26, test_size = 0.2 ==> Average return per 100zł: 89.552, Average accuracy score = 27.272%\n",
      "Most probable scenario | max_depth=27, test_size = 0.2 ==> Average return per 100zł: 96.692, Average accuracy score = 37.238%\n",
      "Highest EV scenario    | max_depth=27, test_size = 0.2 ==> Average return per 100zł: 89.573, Average accuracy score = 27.279%\n",
      "Most probable scenario | max_depth=28, test_size = 0.2 ==> Average return per 100zł: 95.844, Average accuracy score = 37.425%\n",
      "Highest EV scenario    | max_depth=28, test_size = 0.2 ==> Average return per 100zł: 89.815, Average accuracy score = 27.327%\n",
      "Most probable scenario | max_depth=29, test_size = 0.2 ==> Average return per 100zł: 95.524, Average accuracy score = 37.351%\n",
      "Highest EV scenario    | max_depth=29, test_size = 0.2 ==> Average return per 100zł: 89.672, Average accuracy score = 27.299%\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "n = 100\n",
    "for i in range(1, 30):\n",
    "    return1, return2 = (0,0)\n",
    "    accuracy1, accuracy2 = (0,0)\n",
    "    for j in range(n):\n",
    "        return1, accuracy1 = ExpectedReturnNaive(DecisionTreeClassifier, \"entropy\", odds1, X, y, i, j, test_size, return1, accuracy1)\n",
    "        return2, accuracy2 = ExpectedReturnAdvanced(DecisionTreeClassifier, \"entropy\", odds1, X, y, i, j, test_size, return2, accuracy2)\n",
    "    print(f\"Most probable scenario | max_depth={i}, test_size = {test_size} ==> Average return per 100zł: {(return1/n):.3f}, Average accuracy score = {(accuracy1/n):.3f}%\") \n",
    "    print(f\"Highest EV scenario    | max_depth={i}, test_size = {test_size} ==> Average return per 100zł: {(return2/n):.3f}, Average accuracy score = {(accuracy2/n):.3f}%\")\n",
    "\n",
    "#Here we're defining a random_state, because we want reproducibility - crucial for cross examination.\n",
    "#We repeat the algorithm n times, because, well, we don't want to hop onto a bad model just because it got a lucky test sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959ce6c",
   "metadata": {},
   "source": [
    "As we can see, the \"naive\" solution doesn't do that bad - it's not profitable, but it performs better than average, especially for max_depth between 3 and 12. However, the supposedly 'advanced\" algorithm performs horrifyingly badly.\n",
    "\n",
    "How come?\n",
    "- We are betting on less probable scenarios, but ones that wield larger returns. Hence, its' accuracy score is (and should be) below 33%; the issue may be that it shouldn't be that low (it oscilates between 27 and 30%)\n",
    "- Or maybe the probability scores are inaccurate. If a scenario with odds = 12.00 has a 20% chance of occuring, we should definitely bet on it. But 5%? That's why these values matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c63e9",
   "metadata": {},
   "source": [
    "#### Let's try other models: This time, we'll go with a Random Forest. Since it takes way longer to compute, we won't make as many repetitions this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bca3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable scenario | max_depth=1, test_size = 0.25 ==> Average return per 100zł: 98.675, Average accuracy score = 43.970%\n",
      "Most probable scenario | max_depth=2, test_size = 0.25 ==> Average return per 100zł: 98.306, Average accuracy score = 44.402%\n",
      "Most probable scenario | max_depth=3, test_size = 0.25 ==> Average return per 100zł: 97.205, Average accuracy score = 44.849%\n",
      "Most probable scenario | max_depth=4, test_size = 0.25 ==> Average return per 100zł: 96.889, Average accuracy score = 44.973%\n",
      "Most probable scenario | max_depth=5, test_size = 0.25 ==> Average return per 100zł: 96.933, Average accuracy score = 44.966%\n",
      "Most probable scenario | max_depth=6, test_size = 0.25 ==> Average return per 100zł: 96.409, Average accuracy score = 44.773%\n",
      "Most probable scenario | max_depth=7, test_size = 0.25 ==> Average return per 100zł: 97.026, Average accuracy score = 44.207%\n",
      "Most probable scenario | max_depth=8, test_size = 0.25 ==> Average return per 100zł: 98.309, Average accuracy score = 43.514%\n",
      "Most probable scenario | max_depth=9, test_size = 0.25 ==> Average return per 100zł: 98.153, Average accuracy score = 42.931%\n",
      "Most probable scenario | max_depth=10, test_size = 0.25 ==> Average return per 100zł: 97.155, Average accuracy score = 42.290%\n",
      "Most probable scenario | max_depth=11, test_size = 0.25 ==> Average return per 100zł: 96.619, Average accuracy score = 41.827%\n",
      "Most probable scenario | max_depth=12, test_size = 0.25 ==> Average return per 100zł: 96.070, Average accuracy score = 41.352%\n",
      "Most probable scenario | max_depth=13, test_size = 0.25 ==> Average return per 100zł: 96.933, Average accuracy score = 40.920%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[195]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m accuracy1, accuracy2 = (\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     return1, accuracy1 = \u001b[43mExpectedReturnNaive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRandomForestClassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mentropy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43modds1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m#return2, accuracy2 = ExpectedReturnAdvanced(RandomForestClassifier, \"entropy\", odds1, X, y, i, j, test_size, return2, accuracy2)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMost probable scenario | max_depth=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, test_size = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ==> Average return per 100zł: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(return1/n)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Average accuracy score = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(accuracy1/n)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m) \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[183]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mExpectedReturnNaive\u001b[39m\u001b[34m(__Classifier__, __criterion__, odds, X, y, i, j, t, wyg, acc)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mExpectedReturnNaive\u001b[39m(__Classifier__, __criterion__, odds, X, y, i, j, t, wyg, acc):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     X_trainset, X_testset, y_trainset, y_testset = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     meczTree = __Classifier__(criterion=__criterion__, max_depth = i) \u001b[38;5;66;03m#From my observations, criterion type's impact is extremely marginal\u001b[39;00m\n\u001b[32m      4\u001b[39m     meczTree.fit(X_trainset,y_trainset) \u001b[38;5;66;03m#Model training ^^^\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karol\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karol\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2950\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2947\u001b[39m xp, _, device = get_namespace_and_device(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m   2948\u001b[39m train, test = move_to(train, test, xp=xp, device=device)\n\u001b[32m-> \u001b[39m\u001b[32m2950\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   2951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2952\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[32m   2953\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2954\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karol\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2952\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   2947\u001b[39m xp, _, device = get_namespace_and_device(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m   2948\u001b[39m train, test = move_to(train, test, xp=xp, device=device)\n\u001b[32m   2950\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2951\u001b[39m     chain.from_iterable(\n\u001b[32m-> \u001b[39m\u001b[32m2952\u001b[39m         (\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m, _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[32m   2953\u001b[39m     )\n\u001b[32m   2954\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karol\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:357\u001b[39m, in \u001b[36m_safe_indexing\u001b[39m\u001b[34m(X, indices, axis)\u001b[39m\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    349\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSpecifying the columns using strings is only supported for dataframes.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    350\u001b[39m     )\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[33m\"\u001b[39m\u001b[33miloc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    353\u001b[39m     \u001b[38;5;66;03m# TODO: we should probably use is_pandas_df_or_series(X) instead but:\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;66;03m# 1) Currently, it (probably) works for dataframes compliant to pandas' API.\u001b[39;00m\n\u001b[32m    355\u001b[39m     \u001b[38;5;66;03m# 2) Updating would require updating some tests such as\u001b[39;00m\n\u001b[32m    356\u001b[39m     \u001b[38;5;66;03m#    test_train_test_split_mock_pandas.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pandas_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_polars_df_or_series(X):\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _polars_indexing(X, indices, indices_dtype, axis=axis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karol\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:74\u001b[39m, in \u001b[36m_pandas_indexing\u001b[39m\u001b[34m(X, key, key_dtype, axis)\u001b[39m\n\u001b[32m     69\u001b[39m     key = np.asarray(key)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key_dtype == \u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m np.isscalar(key)):\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# using take() instead of iloc[] ensures the return value is a \"proper\"\u001b[39;00m\n\u001b[32m     73\u001b[39m     \u001b[38;5;66;03m# copy that will not raise SettingWithCopyWarning\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m     \u001b[38;5;66;03m# check whether we should index with loc or iloc\u001b[39;00m\n\u001b[32m     77\u001b[39m     indexer = X.iloc \u001b[38;5;28;01mif\u001b[39;00m key_dtype == \u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X.loc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karol\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\generic.py:4155\u001b[39m, in \u001b[36mNDFrame.take\u001b[39m\u001b[34m(self, indices, axis, **kwargs)\u001b[39m\n\u001b[32m   4150\u001b[39m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[32m   4151\u001b[39m     indices = np.arange(\n\u001b[32m   4152\u001b[39m         indices.start, indices.stop, indices.step, dtype=np.intp\n\u001b[32m   4153\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4155\u001b[39m new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4157\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4158\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4159\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4160\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes).__finalize__(\n\u001b[32m   4161\u001b[39m     \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mtake\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4162\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karol\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:910\u001b[39m, in \u001b[36mBaseBlockManager.take\u001b[39m\u001b[34m(self, indexer, axis, verify)\u001b[39m\n\u001b[32m    907\u001b[39m \u001b[38;5;66;03m# Caller is responsible for ensuring indexer annotation is accurate\u001b[39;00m\n\u001b[32m    909\u001b[39m n = \u001b[38;5;28mself\u001b[39m.shape[axis]\n\u001b[32m--> \u001b[39m\u001b[32m910\u001b[39m indexer = \u001b[43mmaybe_convert_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    912\u001b[39m new_labels = \u001b[38;5;28mself\u001b[39m.axes[axis].take(indexer)\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reindex_indexer(\n\u001b[32m    914\u001b[39m     new_axis=new_labels,\n\u001b[32m    915\u001b[39m     indexer=indexer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    918\u001b[39m     copy=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    919\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karol\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\pandas\\core\\indexers\\utils.py:275\u001b[39m, in \u001b[36mmaybe_convert_indices\u001b[39m\u001b[34m(indices, n, verify)\u001b[39m\n\u001b[32m    272\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.empty(\u001b[32m0\u001b[39m, dtype=np.intp)\n\u001b[32m    274\u001b[39m mask = indices < \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmask\u001b[49m\u001b[43m.\u001b[49m\u001b[43many\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    276\u001b[39m     indices = indices.copy()\n\u001b[32m    277\u001b[39m     indices[mask] += n\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karol\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\numpy\\_core\\_methods.py:63\u001b[39m, in \u001b[36m_any\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_any\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m umr_any(a, axis, dtype, out, keepdims, where=where)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "test_size = 0.25\n",
    "n = 100\n",
    "for i in range(1,6):\n",
    "    return1, return2 = (0,0)\n",
    "    accuracy1, accuracy2 = (0,0)\n",
    "    for j in range(n):\n",
    "        return1, accuracy1 = ExpectedReturnNaive(RandomForestClassifier, \"entropy\", odds1, X, y, i, j, test_size, return1, accuracy1)\n",
    "        #return2, accuracy2 = ExpectedReturnAdvanced(RandomForestClassifier, \"entropy\", odds1, X, y, i, j, test_size, return2, accuracy2)\n",
    "    print(f\"Most probable scenario | max_depth={i}, test_size = {test_size} ==> Average return per 100zł: {(return1/n):.3f}, Average accuracy score = {(accuracy1/n):.3f}%\") \n",
    "    #print(f\"Highest EV scenario    | max_depth={i}, test_size = {test_size} ==> Average return per 100zł: {(return2/n):.3f}, Average accuracy score = {(accuracy2/n):.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7a01f",
   "metadata": {},
   "source": [
    "Well that... doesn't look too bad. Still not profitable, but the test size and n repetitions are so large here that variance should be miniscule, and yet it's still not that far off from profitability. I mean, if you pick \"correct\" parameters, like n=20 and test_size = 0.05, you may find a 16% edge - potentially effective for convincing shareholders that your ideas are very good, a bit less efficient for holding onto your job.\n",
    "\n",
    "If you want to learn why I commented out the \"advanced\" algorithm, you can uncomment it and find out yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc23e8b6",
   "metadata": {},
   "source": [
    "#### Let's now check how XGBoost performs. It has a different syntax, so we can't just copy previously defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a9557ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoostPredictionsNaive(oddsy, X, y, t, x, i, j, wyg, acc):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=t, random_state=j)\n",
    "    xgb_train = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "    xgb_test = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n",
    "    params = {\n",
    "        'objective': 'multi:softmax', #Puts observations into one of 3 categories\n",
    "        'max_depth': i,\n",
    "        'learning_rate': x,\n",
    "        'num_class': 3\n",
    "    }\n",
    "    model = xgb.train(params=params, dtrain=xgb_train, num_boost_round=50)\n",
    "    xgb_predictions = model.predict(xgb_test)\n",
    "    xgb_predictions = xgb_predictions.astype(int)\n",
    "    indices = list(X_test.index)\n",
    "    oddsy_xgb_predictions = oddsy.loc[indices]\n",
    "    oddsy_xgb_predictions = oddsy_xgb_predictions.reset_index(drop=True)\n",
    "    oddsy_xgb_predictions.insert(4, 'Predicted_Winner', xgb_predictions)\n",
    "    tabelka = oddsy_xgb_predictions\n",
    "    tabelka.loc[tabelka.Kto_wygrał == tabelka.Predicted_Winner,'Wygrana'] = 1\n",
    "    tabelka.loc[tabelka.Kto_wygrał == 0,'Wygrana'] = tabelka.Oddsy_gospodarze * 100 * tabelka.Wygrana\n",
    "    tabelka.loc[tabelka.Kto_wygrał == 1,'Wygrana'] = tabelka.Oddsy_remis * 100 * tabelka.Wygrana\n",
    "    tabelka.loc[tabelka.Kto_wygrał == 2,'Wygrana'] = tabelka.Oddsy_goście * 100 * tabelka.Wygrana\n",
    "    tabelka['Wygrana'] = tabelka['Wygrana'].fillna(0)\n",
    "    tabelka['Wygrana'] = tabelka['Wygrana'].astype(int)\n",
    "\n",
    "    wyg = wyg + (tabelka['Wygrana'].sum())/(len(tabelka['Wygrana']))\n",
    "    acc = acc + 100*(np.count_nonzero(tabelka['Wygrana']))/len(tabelka['Wygrana'])\n",
    "\n",
    "    return wyg, acc\n",
    "\n",
    "\n",
    "\n",
    "def XGBoostPredictionsAdvanced(oddsy, X, y, t, x, i, j, wyg, acc): #Let's try again\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=t, random_state=j)\n",
    "    xgb_train = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "    xgb_test = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n",
    "    params = {\n",
    "        'objective': 'multi:softprob', #Predicts a probability score of each scenario\n",
    "        'max_depth': i,\n",
    "        'learning_rate': x,\n",
    "        'num_class': 3\n",
    "    }\n",
    "    model = xgb.train(params=params, dtrain=xgb_train, num_boost_round=50)\n",
    "    xgb_predictions = model.predict(xgb_test)\n",
    "    xgb_predictions = pd.DataFrame(xgb_predictions)\n",
    "    indices = list(X_test.index)\n",
    "    oddsy_xgb_predictions = oddsy.loc[indices]\n",
    "    oddsy_xgb_predictions = oddsy_xgb_predictions.reset_index(drop=True)\n",
    "    tabelka = oddsy_xgb_predictions.merge(xgb_predictions, left_index=True, right_index=True)\n",
    "    tabelka['Potencjalny_return'] = 0 #I thought of putting everything below this part into a separate function, but it caused more issues than I was bothered to fix. Maybe later. It's not necessary, not if we don't have to maintain too much of this code\n",
    "    tabelka['Potencjalny_return'] = np.where(tabelka['Kto_wygrał'] == 0, tabelka['Oddsy_gospodarze']*100, tabelka['Potencjalny_return'])\n",
    "    tabelka['Potencjalny_return'] = np.where(tabelka['Kto_wygrał'] == 1, tabelka['Oddsy_remis']*100, tabelka['Potencjalny_return'])\n",
    "    tabelka['Potencjalny_return'] = np.where(tabelka['Kto_wygrał'] == 2, tabelka['Oddsy_goście']*100, tabelka['Potencjalny_return'])\n",
    "    tabelka['Potencjalny_return'] = tabelka['Potencjalny_return'].astype(int)\n",
    "    tabelka['Return_gospodarze'] = tabelka['Oddsy_gospodarze'] * tabelka[0]\n",
    "    tabelka['Return_remis'] = tabelka['Oddsy_remis'] * tabelka[1]\n",
    "    tabelka['Return_goście'] = tabelka['Oddsy_goście'] * tabelka[2]\n",
    "    tabelka[\"Max(EV)\"] = tabelka[['Return_gospodarze', 'Return_remis', 'Return_goście']].max(axis=1)\n",
    "    tabelka['Max(EV)'] = np.where(tabelka['Max(EV)'] == tabelka['Return_gospodarze'], 0, tabelka['Max(EV)'])\n",
    "    tabelka['Max(EV)'] = np.where(tabelka['Max(EV)'] == tabelka['Return_remis'], 1, tabelka['Max(EV)'])\n",
    "    tabelka['Max(EV)'] = np.where(tabelka['Max(EV)'] == tabelka['Return_goście'], 2, tabelka['Max(EV)'])\n",
    "    tabelka['Max(EV)'] = tabelka['Max(EV)'].astype(int)\n",
    "    tabelka['Actual_return'] = np.where(tabelka['Max(EV)'] == tabelka['Kto_wygrał'], tabelka['Potencjalny_return'], 0)\n",
    "    return_calkowity = tabelka['Actual_return'].sum()\n",
    "    wyg = wyg + return_calkowity / len(tabelka)\n",
    "    acc = acc + 100*(np.count_nonzero(tabelka['Actual_return']))/len(tabelka['Actual_return'])\n",
    "\n",
    "    return wyg, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a5a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoostPredictionsNaive    | Average return per 100zł for test_size = 0.2, learning_rate = 0.001, max_depth = 5:  99.068zł,  with accuracy_score = 44.310%\n",
      "XGBoostPredictionsAdvanced | Average return per 100zł for test_size = 0.2, learning_rate = 0.001, max_depth = 5:  90.995zł,  with accuracy_score = 24.680%\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "n = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "#for i in range(1,6):    Not needed when learning_rate is so low; other values will just yield the same result. We can tune other parameters as well, but that will most likely keep us around 99%\n",
    "wygrana1, wygrana2 = (0,0)\n",
    "accuracy1, accuracy2 = (0,0)\n",
    "for j in range(n):\n",
    "    wygrana1, accuracy1 = XGBoostPredictionsNaive(odds1, X, y, test_size, learning_rate, 1, j, wygrana1, accuracy1)\n",
    "    wygrana2, accuracy2 = XGBoostPredictionsAdvanced(odds1, X, y, test_size, learning_rate, 1, j, wygrana2, accuracy2)\n",
    "print(f\"XGBoostPredictionsNaive    | Average return per 100zł for test_size = {test_size}, learning_rate = {learning_rate}, max_depth = {i}:  {wygrana1/n:.3f}zł,  with accuracy_score = {accuracy1/n:.3f}%\")\n",
    "print(f\"XGBoostPredictionsAdvanced | Average return per 100zł for test_size = {test_size}, learning_rate = {learning_rate}, max_depth = {i}:  {wygrana2/n:.3f}zł,  with accuracy_score = {accuracy2/n:.3f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f9d04d",
   "metadata": {},
   "source": [
    "Well that's just embarassing for the so-called \"advanced\" algorithm. Percentage scores must be totally off.\n",
    "\n",
    "At the same time, again, the \"naive\" algorithm edges close to profitability, but it's so close yet so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8f2844",
   "metadata": {},
   "source": [
    "There's also LogisticRegression that was worth trying out, but honestly, results are comparable to how decision trees performed, i.e. that would bring exactly nothing to the table.\n",
    "\"Naive\" score oscilates around 97%, whereas \"advanced\" sits close to 93%. Again, not even better than random selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c002184",
   "metadata": {},
   "source": [
    "#### Some conclusions:\n",
    "- The \"naive\" algorithm is... not bad. 99% return doesn't sound too impressive, but keep in mind that EV of playing this game is 94.3%, meaning that if this were a zero-sum game, this algorithm would actually find a 5% edge.\n",
    " Especially impressive after considering the fact that the only data we are working with here, is each team's performance during the previous season. That's it.\n",
    "- If this model turned out to be profitable, then it'd be exceptionally easy to maintain. Data insertion would only have to occur once per year.\n",
    "- The \"advanced\" algorithm is obviously bad. Something must be off with percentage scores, it does matter if a 40%, 35%, 25% triplet of scenarios is predicted to be a 45%, 40%, 15%. We can see it by how low the accuracy score is; to some extent, it should be lower than 33%, because maximizing EV means that sometimes we should bet for the underdogs. But it can't be as low as 24%.\n",
    "- I've attempted to calibrate percentage scores using a CalibratedClassifierCV, but with little result. I might come back later and try again.\n",
    "- Don't gamble. Unless you have a verifiably profitable model. Then do gamble. It might turn out to be a safer investment than buying MAG-7 stocks (maybe not)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
